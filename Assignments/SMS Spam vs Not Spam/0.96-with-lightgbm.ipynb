{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9539002f",
   "metadata": {
    "papermill": {
     "duration": 0.013539,
     "end_time": "2022-01-12T06:43:14.413601",
     "exception": false,
     "start_time": "2022-01-12T06:43:14.400062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SPAM vs NOT-SPAM with LigthGBM\n",
    "\n",
    "In this notebook, I am using classical Machine Learning method to classify spam messages. The model used is LightGBM which achieves an F1-Score of 0.96. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d16809",
   "metadata": {
    "papermill": {
     "duration": 0.011799,
     "end_time": "2022-01-12T06:43:14.437992",
     "exception": false,
     "start_time": "2022-01-12T06:43:14.426193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32df286",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:14.470191Z",
     "iopub.status.busy": "2022-01-12T06:43:14.469632Z",
     "iopub.status.idle": "2022-01-12T06:43:16.870112Z",
     "shell.execute_reply": "2022-01-12T06:43:16.870623Z",
     "shell.execute_reply.started": "2022-01-12T06:40:07.453772Z"
    },
    "papermill": {
     "duration": 2.420924,
     "end_time": "2022-01-12T06:43:16.870934",
     "exception": false,
     "start_time": "2022-01-12T06:43:14.450010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import scipy\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74517f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:16.901596Z",
     "iopub.status.busy": "2022-01-12T06:43:16.900987Z",
     "iopub.status.idle": "2022-01-12T06:43:16.903571Z",
     "shell.execute_reply": "2022-01-12T06:43:16.903138Z",
     "shell.execute_reply.started": "2022-01-12T06:40:08.372720Z"
    },
    "papermill": {
     "duration": 0.019742,
     "end_time": "2022-01-12T06:43:16.903704",
     "exception": false,
     "start_time": "2022-01-12T06:43:16.883962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4988ac",
   "metadata": {
    "papermill": {
     "duration": 0.012242,
     "end_time": "2022-01-12T06:43:16.928416",
     "exception": false,
     "start_time": "2022-01-12T06:43:16.916174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Defining the configurations for training and the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2c0c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:16.959961Z",
     "iopub.status.busy": "2022-01-12T06:43:16.958860Z",
     "iopub.status.idle": "2022-01-12T06:43:16.961017Z",
     "shell.execute_reply": "2022-01-12T06:43:16.961517Z",
     "shell.execute_reply.started": "2022-01-12T06:40:12.971521Z"
    },
    "papermill": {
     "duration": 0.020341,
     "end_time": "2022-01-12T06:43:16.961690",
     "exception": false,
     "start_time": "2022-01-12T06:43:16.941349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'num_folds': 6,\n",
    "    'seed': 541,\n",
    "    'TARGET': 'target',\n",
    "}\n",
    "\n",
    "LGBM_PARAMS = {\n",
    "    'seed': Config['seed'],\n",
    "    'objective': 'binary',\n",
    "    'early_stopping_round': 1000,\n",
    "    'verbosity': -1,\n",
    "    'n_estimators': 2000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fd8c2",
   "metadata": {
    "papermill": {
     "duration": 0.012685,
     "end_time": "2022-01-12T06:43:16.987347",
     "exception": false,
     "start_time": "2022-01-12T06:43:16.974662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following function is used to create K-Fold split on the data, stratified on the target variable. For reproducbility, we will shuffle the dataset with a Seed before creating the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d243edd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.016381Z",
     "iopub.status.busy": "2022-01-12T06:43:17.015525Z",
     "iopub.status.idle": "2022-01-12T06:43:17.020300Z",
     "shell.execute_reply": "2022-01-12T06:43:17.020811Z",
     "shell.execute_reply.started": "2022-01-12T06:40:13.514810Z"
    },
    "papermill": {
     "duration": 0.020715,
     "end_time": "2022-01-12T06:43:17.020982",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.000267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_folds(data):\n",
    "    \n",
    "    data['kfold'] = -1\n",
    "    \n",
    "    kf = model_selection.StratifiedKFold(n_splits=Config['num_folds'], shuffle=True, random_state=Config['seed'])\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data[Config['TARGET']])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c52c1b",
   "metadata": {
    "papermill": {
     "duration": 0.012285,
     "end_time": "2022-01-12T06:43:17.045554",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.033269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This function basically returns the sparse form of Tf-Idf vectors. \n",
    "\n",
    "* I experimented with different *n_gram* ranges but the performance is better without it. \n",
    "* Moreover, I am not using the *max_features* parameter because I am using sparse form and the text samples are small, so no need to limit the number of features.\n",
    "* Tf-Idf will remove Stop Words before computing the vectors. I have used the Tf-Idf parameter to removing the English Stop Words.\n",
    "* Lemmatization is not helping. Score dips by 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563b2c08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.074306Z",
     "iopub.status.busy": "2022-01-12T06:43:17.073406Z",
     "iopub.status.idle": "2022-01-12T06:43:17.080385Z",
     "shell.execute_reply": "2022-01-12T06:43:17.080869Z",
     "shell.execute_reply.started": "2022-01-12T06:40:13.947847Z"
    },
    "papermill": {
     "duration": 0.022818,
     "end_time": "2022-01-12T06:43:17.081042",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.058224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    mtokenizer = MWETokenizer()\n",
    "    mwe = mtokenizer.tokenize(text.split())\n",
    "    words =[]\n",
    "    for t in mwe:\n",
    "        if t.isalpha():\n",
    "            words.append(t)\n",
    "    return words\n",
    "\n",
    "def lemmatize(text): \n",
    "    tokens = tokenize(text)\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    return ' '.join(lemmatized_sentence)\n",
    "\n",
    "def vectorize(train_text, test_text, lemmatize=False):\n",
    "    if lemmatize:\n",
    "        for i, text in enumerate(train_text):\n",
    "            train_text[i] = lemmatize(text)\n",
    "        for i, text in enumerate(test_text):\n",
    "            test_text[i] = lemmatize(text)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    train_vectors = vectorizer.fit_transform(train_text) # .toarray() \n",
    "    test_vectors = vectorizer.transform(test_text) # .toarray()\n",
    "    \n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346a794",
   "metadata": {
    "papermill": {
     "duration": 0.012335,
     "end_time": "2022-01-12T06:43:17.105733",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.093398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* The dataset is highly imbalaced. Hence, a powerful metric to evaluate the model is F1-Score. The *get_score* function evaluates the predictions and returns the F1-Score. If *report* parameter is set to *True*, it also prints the Classification Report. \n",
    "* The *lgb_f1_score* is the custom metric funcion for the LightGBM model to utilize it's **early_stopping** feature. (There isn't a default option).\n",
    "* *train_fn* takes the training data and trains the model for 1 Fold. As in, if Fold 0 is given, the model will be trained on Folds 1, 2, 3, 4, 5 and evaluated on the unseed Fold 0. This exercise is repeated for all folds.\n",
    "* The *run* function just loops over to train for all folds and calculate the average score for all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc7631d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.136579Z",
     "iopub.status.busy": "2022-01-12T06:43:17.135627Z",
     "iopub.status.idle": "2022-01-12T06:43:17.146469Z",
     "shell.execute_reply": "2022-01-12T06:43:17.146948Z",
     "shell.execute_reply.started": "2022-01-12T06:40:14.388024Z"
    },
    "papermill": {
     "duration": 0.0281,
     "end_time": "2022-01-12T06:43:17.147113",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.119013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_preds, report=False):\n",
    "    y_true = [id_to_class[i] for i in y_true]\n",
    "    y_preds = [id_to_class[i] for i in y_preds]\n",
    "    \n",
    "    if report:\n",
    "        print(metrics.classification_report(y_true, y_preds))  \n",
    "    return round(metrics.f1_score(y_true, y_preds, average='macro'), 2)\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = (y_hat > 0.5) * 1 \n",
    "    return 'f1', get_score(y_true, y_hat), True\n",
    "\n",
    "def train_fn(train_vectors, train_folds, fold):\n",
    "    train_indices = train_folds['kfold'] != fold\n",
    "    valid_indices = train_folds['kfold'] == fold\n",
    "    \n",
    "    x_train = train_vectors[train_indices]\n",
    "    y_train = train_folds[Config['TARGET']][train_indices].values\n",
    "    \n",
    "    x_valid = train_vectors[valid_indices]\n",
    "    y_valid = train_folds[Config['TARGET']][valid_indices].values\n",
    "        \n",
    "    lgb_train = lgb.Dataset(x_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "    \n",
    "    model = lgb.train(LGBM_PARAMS,  lgb_train, valid_sets=[lgb_valid], feval=lgb_f1_score, verbose_eval=False)\n",
    "    y_preds = model.predict(x_valid)\n",
    "\n",
    "    y_preds = (y_preds > 0.5) * 1\n",
    "    \n",
    "    score = get_score(y_valid, y_preds)\n",
    "    print(f'Fold: {fold}, Score: {score}')\n",
    "    \n",
    "    joblib.dump(model, f'model_{fold}.bin')\n",
    "    \n",
    "    del model\n",
    "    del train_indices, valid_indices\n",
    "    del x_train, y_train, x_valid, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def run(train_vectors, train_folds): \n",
    "    score_avg = 0\n",
    "    for fold in range(Config['num_folds']):\n",
    "        score_avg += train_fn(train_vectors, train_folds, fold)\n",
    "        \n",
    "    score_avg /= Config['num_folds']\n",
    "    print('Average Score', round(score_avg, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3594a2",
   "metadata": {
    "papermill": {
     "duration": 0.01252,
     "end_time": "2022-01-12T06:43:17.172866",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.160346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Importing the data. Encoding used while reading the file is *latin-1* because the text in the file causes an error while using *utf* encoding (mainly because of the commas in the text, ugh!)\n",
    "\n",
    "* The dataset has weird column names. Firstly, I am just going to rename the features.\n",
    "* I have created the *class_to_id* and *id_to_class* dictionary that maps the target to a numeric encoding and vice-versa.\n",
    "* I am also creating 2 extra features: *num_words* has the number of words in the text calculated using the python .split() function and *num_characters* has the number of characters in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da03283c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.202092Z",
     "iopub.status.busy": "2022-01-12T06:43:17.201166Z",
     "iopub.status.idle": "2022-01-12T06:43:17.268007Z",
     "shell.execute_reply": "2022-01-12T06:43:17.267309Z",
     "shell.execute_reply.started": "2022-01-12T06:40:14.862880Z"
    },
    "papermill": {
     "duration": 0.082389,
     "end_time": "2022-01-12T06:43:17.268153",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.185764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\\\n",
    "        .rename(columns={'v1': 'target', 'v2': 'text'})[['target', 'text']]\n",
    "\n",
    "df['num_words'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['num_characters'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "class_to_id = {'ham': 0, 'spam': 1}\n",
    "id_to_class = {id_: class_ for class_, id_ in class_to_id.items()}\n",
    "\n",
    "df['target'] = df['target'].map(class_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa6c06",
   "metadata": {
    "papermill": {
     "duration": 0.012623,
     "end_time": "2022-01-12T06:43:17.293836",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.281213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am splitting the data to leave out 0.15 of the entire data for testing the model. These samples will never be seen by the model. Using a seed to for reproducible results. The K-Fold split will be computed on the remaing 0.85 of the data. I have chosen K = 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c6cf66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.323039Z",
     "iopub.status.busy": "2022-01-12T06:43:17.322440Z",
     "iopub.status.idle": "2022-01-12T06:43:17.340925Z",
     "shell.execute_reply": "2022-01-12T06:43:17.341426Z",
     "shell.execute_reply.started": "2022-01-12T06:40:15.293658Z"
    },
    "papermill": {
     "duration": 0.034308,
     "end_time": "2022-01-12T06:43:17.341597",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.307289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA, TEST_DATA = model_selection.train_test_split(\n",
    "    df, \n",
    "    test_size=0.15, \n",
    "    stratify=df[Config['TARGET']].values,\n",
    "    random_state=Config['seed'],\n",
    ")\n",
    "\n",
    "TRAIN_DATA = TRAIN_DATA.reset_index(drop=True)\n",
    "TEST_DATA = TEST_DATA.reset_index(drop=True)\n",
    "\n",
    "TRAIN_DATA = create_folds(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ad6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T14:26:49.410035Z",
     "iopub.status.busy": "2022-01-10T14:26:49.409778Z",
     "iopub.status.idle": "2022-01-10T14:26:49.520416Z",
     "shell.execute_reply": "2022-01-10T14:26:49.519435Z",
     "shell.execute_reply.started": "2022-01-10T14:26:49.409994Z"
    },
    "papermill": {
     "duration": 0.012417,
     "end_time": "2022-01-12T06:43:17.366586",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.354169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Using the *vectorize* function defined above, I will get the sparse form of train and test vectors. \n",
    "* Then, since I want to use *num_words* and *num_characters* features, I will convert them to sparse matrices too.\n",
    "* I will concat the vectors with the sparse form of the 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704e2f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.394645Z",
     "iopub.status.busy": "2022-01-12T06:43:17.394078Z",
     "iopub.status.idle": "2022-01-12T06:43:17.507457Z",
     "shell.execute_reply": "2022-01-12T06:43:17.507953Z",
     "shell.execute_reply.started": "2022-01-12T06:40:15.788156Z"
    },
    "papermill": {
     "duration": 0.12895,
     "end_time": "2022-01-12T06:43:17.508119",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.379169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_vectors, test_vectors = vectorize(TRAIN_DATA.text.tolist(), TEST_DATA.text.tolist())\n",
    "\n",
    "train_sparse = scipy.sparse.csr_matrix(TRAIN_DATA[['num_words', 'num_characters']].values)\n",
    "test_sparse = scipy.sparse.csr_matrix(TEST_DATA[['num_words', 'num_characters']].values)\n",
    "\n",
    "train_sparse = scipy.sparse.hstack([train_sparse, train_vectors]).tocsr()\n",
    "test_sparse = scipy.sparse.hstack([test_sparse, test_vectors]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08388d",
   "metadata": {
    "papermill": {
     "duration": 0.012515,
     "end_time": "2022-01-12T06:43:17.533480",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.520965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let the training begin!\n",
    "\n",
    "The model has achieved an Average Cross-Validation F1-Score of 0.96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da73cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:43:17.564695Z",
     "iopub.status.busy": "2022-01-12T06:43:17.564074Z",
     "iopub.status.idle": "2022-01-12T06:44:08.793237Z",
     "shell.execute_reply": "2022-01-12T06:44:08.793698Z",
     "shell.execute_reply.started": "2022-01-12T06:40:16.234941Z"
    },
    "papermill": {
     "duration": 51.247558,
     "end_time": "2022-01-12T06:44:08.793868",
     "exception": false,
     "start_time": "2022-01-12T06:43:17.546310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0, Score: 0.96\n",
      "Fold: 1, Score: 0.95\n",
      "Fold: 2, Score: 0.97\n",
      "Fold: 3, Score: 0.95\n",
      "Fold: 4, Score: 0.96\n",
      "Fold: 5, Score: 0.95\n",
      "Average Score 0.96\n"
     ]
    }
   ],
   "source": [
    "run(train_sparse, TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20c122",
   "metadata": {
    "papermill": {
     "duration": 0.013623,
     "end_time": "2022-01-12T06:44:08.821508",
     "exception": false,
     "start_time": "2022-01-12T06:44:08.807885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Inference Time.\n",
    "\n",
    "* I will load the trained models, calulate the probabilites and keep adding them to *test_preds*. \n",
    "* Dividing it by *num_folds* will give the average predicted probabilities.\n",
    "* When these predictions are evaluated, the F1-Score is 0.96.  \n",
    "\n",
    "**This means that the CV score is the same as the Test Score. That's a great achievement as it means there's no overfittig and the model is performing well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc6d58c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:44:08.854367Z",
     "iopub.status.busy": "2022-01-12T06:44:08.853285Z",
     "iopub.status.idle": "2022-01-12T06:44:08.906228Z",
     "shell.execute_reply": "2022-01-12T06:44:08.906754Z",
     "shell.execute_reply.started": "2022-01-12T06:41:28.641503Z"
    },
    "papermill": {
     "duration": 0.070892,
     "end_time": "2022-01-12T06:44:08.906966",
     "exception": false,
     "start_time": "2022-01-12T06:44:08.836074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'model_{}.bin'\n",
    "test_preds = np.zeros((TEST_DATA.shape[0]))\n",
    "\n",
    "for fold in range(Config['num_folds']):\n",
    "    model = joblib.load(model_path.format(fold))    \n",
    "    test_preds += model.predict(test_sparse)\n",
    "\n",
    "test_preds /= Config['num_folds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0bfc117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T06:44:08.939131Z",
     "iopub.status.busy": "2022-01-12T06:44:08.938220Z",
     "iopub.status.idle": "2022-01-12T06:44:08.955447Z",
     "shell.execute_reply": "2022-01-12T06:44:08.954820Z",
     "shell.execute_reply.started": "2022-01-12T06:41:28.729777Z"
    },
    "papermill": {
     "duration": 0.034041,
     "end_time": "2022-01-12T06:44:08.955585",
     "exception": false,
     "start_time": "2022-01-12T06:44:08.921544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       724\n",
      "        spam       0.97      0.88      0.93       112\n",
      "\n",
      "    accuracy                           0.98       836\n",
      "   macro avg       0.98      0.94      0.96       836\n",
      "weighted avg       0.98      0.98      0.98       836\n",
      "\n",
      "Test F1-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "test_classes = (test_preds > 0.5) * 1\n",
    "print('Test F1-Score:', get_score(TEST_DATA[Config['TARGET']], test_classes, report=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.518674,
   "end_time": "2022-01-12T06:44:09.680202",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-12T06:43:06.161528",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
